\section{Experiment setup}
\label{sec:setup}
During the experiment 5 analysis tools were configured and 3 separate programs were inspected.
The environment for the experiment was a Docker container based on the \textit{ubuntu:18.04} image.
The kernel is that of the host machine, which can be identified by: \textit{6.9.1-arch1-1}.
The test computer had 16GB of RAM and an \textit{AMD Ryzen 7 4800H} 8 core CPU.
The analysis timing information was collected by the \textit{time} command, except for one case, which is specifically stated below. More information about what steps of the process got timed for each of the tools can also be found below.

The 3 programs under analysis are listed below:
\begin{itemize}
    \item {\textbf{ls} - was chosen since it is a simple, frequently used program. It is part of the bigger \textit{coreutils} package, of which version 8.28 was used.}
    \item {\textbf{sqlite} - was chosen since it is a commonly used application database. It is more complicated than \textit{ls} and therefore invokes more system calls. Version 3.22 was used.}
    \item {\textbf{redis-server} - was chosen to test a larger server application. Since it is a server application it also deals with sockets, unlike the previous 2 applications. Version 4.0.9 was used with the default configuration.}
\end{itemize}

The \textit{Dockerfile} for setting up the analysis tools, and patches made to the projects (where applicable) can be all found on the project's source code repository (https://github.com/sbencoding/syscall-sandboxing-research).

\subsection {Dynamic analysis solution}
A simple tracer was designed by the author to test out and report the results of a purely dynamic analysis based solution.
The tracer is based on the \textit{ptrace} system call and requires a Linux kernel version of at least 5.3 to function, since it relies on the \textit{PTRACE\_GET\_SYSCALL\_INFO} operation.

Since dynamic analysis requires the program under test to be exercised in order to explore as many program states as possible, a simple framework and scripts to provide input to and gather the results from the analysed programs is also included. In the case of \textit{ls} and sqlite the input cases are manually determined, while in the case of redis the \textit{redis-benchmark} utility included with the redis-server package was used, with the \textit{-n 10} option.

The dynamic tracer also supports the multi phase execution model in order to be compared with temporal specialization \cite{ref_mp_1}. The tool allows specifying the exact instruction at which a phase transition should be considered, by passing the offset of the instruction from the start of the binary as a command line argument. The tracer supports up to 10 phase transition points, however this is easy to extend if needed.

In the case of \textit{ls} the call instruction to \textit{clear\_files} found within the \textit{main} function was chosen as the phase transition point, since most initialization is done before this point, and the work of listing files and folders is done afterwards.
For sqlite, the first instruction of the \textit{shell\_exec} function was chosen, as this is the part where it starts dealing with the query after loading the database and handling other command line options.
Finally the first instruction of the \textit{aeMain} function is the transition point chosen for redis, which is the same transition point chosen by temporal specialization.

The timing information includes running the simple framework, executing all the defined analysis cases, gathering and printing all results.

The source code for the tracer, as well as the test framework with the test cases used is available in the project's source code repository.

\subsection {sysfilter}
The sysfilter \cite{ref_sp_3} analysis program, which relies mainly on static analysis, was setup inside the docker container using the instructions given by the authors.
After the project was built, the programs under analysis were ran through the compiled analysis tool, and the results were gathered.

The timing information includes only the runtime of the analysis program, which also prints the results.

\subsection {Confine}
The Confine \cite{ref_sp_2} analysis program, which relies mainly on static analysis, was ran on the host machine, outside of a docker container, since the tool itself is going to spawn containers, where the programs under test are ran.
3 docker containers were analysed, one by one, each based on the \textit{ubuntu:18.04} image and running one of the programs under analysis.
Furthermore to make an adequate comparison between the analysis tools the author has modified the source code to include include the syscalls from the program that was being tested and its dependencies, as opposed to all programs the container runs.
It also needs to be mentioned that \textit{sysdig}, a monitoring tool Confine relies on, crashed on the host system when launching a docker container therefore \textit{execsnoop} was used as a \textit{monitoringtool}.

In addition to the instructions of the authors on running the tool, the above mentioned \textit{monitoringtool} options was used, as well as \textit{-d} to get debug output.
The input JSON file that defines which containers to test was a simple file containing only the container which ran a given program under analysis.

The timing information collected includes the time to analyze extracted binaries and dependencies and to generate a list of used system calls. This specifically excludes the time to run the container and extract the binaries and dependencies from it, in order to make a better comparison to the other tools. The time was measured using the \textit{time.time\_ns()} python function.

\subsection {Chestnut}
The Chestnut \cite{ref_sp_1} project, which relies mainly on static analysis, has 2 main components performing analysis. \textit{Sourcealyzer} extracts the used system calls through a compiler pass by patching the LLVM compiler, while the \textit{Binalyzer} scripts operate on already compiled binaries. The project authors provide \textit{Binalyzer} for cases when using \textit{Sourcealyzer} is not feasible, for example because of incompatibility with the LLVM compiler or lack of available source code \cite{ref_sp_1}.

Optionally a second phase of analysis called the \textit{Finalyzer} can be performed either after the aforementioned 2 analysis tools or as a standalone analysis process.
It is based on dynamic tracing, and its goal is to unblock system calls which were not detected by the static analysis tools, such as in the case when the program under analysis dynamically loads another process as a child.
It is intended to be used during development in a safe environment, and requires the tester to manually decide whether a system call should be allowed that was not detected by static analysis.

\textit{Finalyzer} is not evaluated in this research, since it requires manual intervention to decide if a system call outside the list of allowed system calls can be made or not.
Furthermore, to which extent the applications work after blocking the detected system calls is outside the scope of this research and is indicated as future work.

The project was built and ran inside the docker container. The \textit{Binalyzer} part got patched by Petr Khartskhaev, to solve an issue where the \textit{syscalls.py} script did not work.
The experiment evaluates both \textit{syscalls.py} and \textit{cfg.py} on the binaries included with the Ubuntu distribution, as well as the statically built binaries, which are generated by the patched clang compiler explained below.

Furthermore the author has made some modifications to the \textit{Sourcealyzer} part of the project. First, the LLVM download and patch script was modified to apply the patch without user interaction and to also build clang and lld.
Next, the number of concurrent build jobs were also reduced to 10, since the build process ran into out of memory issues on the computer performing the build.

Afterwards \textit{musl} was setup (commit hash 007997299248b8682dcbb73595c53dfe86071c83), since all dependencies of programs under test need to be rebuilt using a new compiler flag, and building \textit{glibc} using the patched clang compiler did not work due to some missing GNU extensions.
Then the \textit{musl-clang} wrapper was modified to use the patched linker, instead of the system default, this is required for the intended program analysis to be done.
Furthermore a copy of the wrapper was made that supports static building, since the default one has command line options which imply dynamic linking.

Next libseccomp was rebuilt (commit hash 9da5d174e3ef219baab020a79c789f2075ace45c) against \textit{musl}, because a chestnut object, that needs to be built into the final analyzed program, depends on it, however the final analyzed program is not built against \textit{glibc} like the installed libseccomp-dev package is.
In the following step the aforementioned chestnut object was rebuilt against the new libseccomp, and that is the final step to setup the tool.

From here the programs under analysis need to be rebuilt, with the \textit{-static -lseccomp -fautosandbox} compiler flags, for the tool to perform analysis and include the used system calls in the resulting binaries.

Timing information for \textit{Sourcalyzer} includes the time to clean build the programs under analysis, using 16 concurrent \textit{make} jobs where applicable. For \textit{Binalyzer} it includes running the given analysis script from start to completion.

\subsection {Temporal Specialization}
The temporal specialization \cite{ref_mp_1} project, which relies mainly on static analysis, was setup and ran inside the docker container.
To setup the container the docker file provided by the project's repository was used as a basis, but with the specific \textit{ubuntu:18.04} base image.
This docker file was modified to download and build the \textit{gold linker}, which is required by the project to generate bitcode, needed for the analysis of binaries.

Furthermore \textit{createSyscallState.py} was modified to properly find the generated CFG files, without the use of the optional \textit{--libdebloating} parameter.
Additionally \textit{run.sh} was modified to only perform the system calls analysis, and do so without the \textit{--libdebloating} flag.
The reasoning behind the changes is to skip the analysis using \textit{libdebloating}, which the original project does for the sake of comparison, but it is a tool not in the scope of this research.
Similarly, statistics about exploit mitigation are disabled, since this research does not consider such a metric.

The pre-generated bitcode for redis was using a different version than this research, therefore a new bitcode file was generated with the same settings, for the desired version.
Bitcode for sqlite was generated, and \textit{shell\_exec} was chosen to be the entry function of the worker phase.
\textit{ls} did not get tested using this tool, due to difficulties of specifying a proper worker phase entry point for this application.

For this project 2 kinds of timings are collected, both using the \textit{time} shell command.
First, the time required to generate the bitcode, which means a full clean build of the tested programs, using 16 concurrent \textit{make} jobs where applicable.
Second, the time required to perform the analysis on the generated bitcode, which includes CFG generations and extracting the used system calls.
